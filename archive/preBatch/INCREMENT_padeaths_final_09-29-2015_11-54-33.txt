./incDriver.py data/padeaths_final.csv -i pre -T -v 1


Using: data/padeaths_final.csv (4974)  --  (0.628955 s)
Initial Clustering: pre
Initial:  --  (0.164990 s)
Accuracy: 4110 of 4974: 82.630 %
H: 0.580722 C: 0.409397 V: 0.480237 JCC: 0.367172

Rows are labels, Columns are Clusters

        0     1    2    3    4    5
  0    43     0    2   46    0    0
  1   116     1  365   92    4    0
  2     1  1128    2   17  577  152
  3    62     0    4   94    5    0
  4  1168     0  697  388    5    5




Class: MergeINCREMENT
	RecursiveOPTICS
	MedoidSelector
	FarthestLabelFeedback
	OracleMatching
	Siamese


Siamese Setup:
	Batch Size: 10
	Output Size: 100
	Train Size: 100000

Testing INCREMENT
Subclustering:
Computing Distance
Running OPTICS: minPts = 5

Subcluster Breakdown:
	0: 6.990590 (1390)
		0: 6.278638 -- 1.340787  (24)
		1: 5.306838 -- 0.810968  (50)
		2: 6.367942 -- 1.583829  (19)
		3: 4.957726 -- 0.983988  (30)
		4: 6.174781 -- 0.934232  (56)
		5: 6.589325 -- 1.385220  (30)
		6: 5.920539 -- 2.125961  (9)
		7: 6.567970 -- 0.999936  (54)
		8: 6.612629 -- 1.042428  (44)
		9: 6.885159 -- 0.959690  (57)
		10: 7.434082 -- 1.452983  (28)
		11: 6.033438 -- 1.642548  (41)
		12: 5.646188 -- 1.418920  (17)
		13: 6.182988 -- 1.105277  (41)
		14: 7.382101 -- 1.216451  (51)
		15: 5.762526 -- 1.545209  (15)
		16: 6.154888 -- 1.106083  (36)
		17: 6.720705 -- 1.638890  (19)
		18: 6.439660 -- 1.002033  (51)
		19: 7.658732 -- 1.678430  (23)
		20: 7.128369 -- 1.657861  (26)
		21: 6.664756 -- 1.134256  (37)
		22: 7.020701 -- 1.113073  (44)
		23: 6.472895 -- 2.683516  (7)
		24: 7.033189 -- 1.395574  (27)
		25: 6.918580 -- 1.281531  (34)
		26: 7.326935 -- 1.197476  (44)
		27: 6.579981 -- 2.710793  (7)
		28: 6.656050 -- 1.410742  (24)
		29: 7.579976 -- 0.642960  (165)
		30: 8.171607 -- 1.584173  (29)
		31: 7.345847 -- 1.238597  (40)
		32: 7.650790 -- 1.135376  (49)
		33: 8.385742 -- 0.935721  (86)
		34: 8.217090 -- 1.715610  (27)
		35: 8.890865 -- 1.506463  (49)
	--> std: 1.497904 -- 0.441070

	1: 5.113596 (1129)
		36: 3.184644 -- 1.300173  (8)
		37: 3.690748 -- 0.661144  (39)
		38: 3.234517 -- 0.623686  (34)
		39: 2.851504 -- 1.014049  (9)
		40: 3.130806 -- 1.016458  (12)
		41: 2.549492 -- 1.476561  (4)
		42: 3.953317 -- 0.618088  (53)
		43: 4.468287 -- 1.191471  (17)
		44: 4.513166 -- 0.952016  (32)
		45: 4.500833 -- 0.941355  (35)
		46: 3.645371 -- 0.837356  (21)
		47: 4.069422 -- 0.947979  (23)
		48: 4.051761 -- 1.378959  (10)
		49: 4.641953 -- 0.561688  (84)
		50: 5.181541 -- 0.782182  (49)
		51: 4.527345 -- 1.045593  (20)
		52: 4.983198 -- 0.655431  (71)
		53: 5.000610 -- 0.640869  (70)
		54: 5.563711 -- 0.820610  (57)
		55: 5.022370 -- 1.015509  (37)
		56: 4.710054 -- 1.446011  (12)
		57: 5.598382 -- 0.738205  (63)
		58: 6.190104 -- 1.475429  (19)
		59: 5.872580 -- 1.325847  (21)
		60: 5.729006 -- 1.775051  (15)
		61: 5.562421 -- 0.983172  (38)
		62: 5.284247 -- 1.770510  (10)
		63: 5.186500 -- 2.117459  (7)
		64: 6.374901 -- 0.848037  (75)
		65: 5.115681 -- 1.071972  (63)
		66: 6.938167 -- 1.150656  (42)
		67: 7.244123 -- 2.238874  (23)
		68: 6.864692 -- 1.767639  (49)
		69: 7.106599 -- 3.101041  (7)
	--> std: 1.466513 -- 0.540298

	2: 6.897547 (1070)
		70: 6.056565 -- 1.528705  (20)
		71: 5.508192 -- 1.481861  (15)
		72: 5.242935 -- 1.336856  (17)
		73: 5.954196 -- 1.276576  (24)
		74: 5.257665 -- 2.366456  (6)
		75: 6.302508 -- 1.468792  (21)
		76: 5.833939 -- 1.139673  (34)
		77: 5.046600 -- 1.562900  (12)
		78: 5.303899 -- 0.842613  (68)
		79: 5.432975 -- 1.326260  (18)
		80: 5.552882 -- 1.432948  (18)
		81: 6.123476 -- 1.516418  (19)
		82: 6.614909 -- 1.416824  (25)
		83: 6.749584 -- 2.071571  (12)
		84: 7.672681 -- 4.430076  (4)
		85: 5.480304 -- 2.238524  (7)
		86: 6.182603 -- 1.404819  (21)
		87: 6.992106 -- 2.011000  (15)
		88: 6.764341 -- 1.513035  (29)
		89: 5.551159 -- 2.105238  (8)
		90: 6.638808 -- 0.969784  (54)
		91: 7.320716 -- 1.810451  (19)
		92: 7.190943 -- 0.911249  (81)
		93: 6.947950 -- 1.058338  (79)
		94: 7.065424 -- 0.885006  (70)
		95: 6.481185 -- 1.810986  (14)
		96: 7.100522 -- 1.779340  (18)
		97: 7.122693 -- 1.420374  (29)
		98: 7.785088 -- 2.308228  (13)
		99: 7.682720 -- 1.139088  (49)
		100: 8.025959 -- 0.809313  (110)
		101: 7.404657 -- 3.124042  (7)
		102: 6.311900 -- 1.440775  (29)
		103: 8.688472 -- 1.083151  (105)
	--> std: 1.642550 -- 0.698940

	3: 7.586960 (637)
		104: 7.486997 -- 8.568831  (633)
		105: 23.406077 -- 13.522767  (4)
	--> std: 8.700196 -- 2.476968

	4: 6.486207 (591)
		106: 3.816001 -- 0.649918  (47)
		107: 5.095997 -- 0.873323  (56)
		108: 6.124382 -- 1.385093  (37)
		109: 6.886143 -- 1.837393  (23)
		110: 6.112597 -- 1.132949  (31)
		111: 6.469685 -- 1.137042  (43)
		112: 5.900191 -- 1.546937  (16)
		113: 6.156641 -- 0.908561  (50)
		114: 6.871798 -- 1.426512  (27)
		115: 7.103472 -- 1.518666  (26)
		116: 6.010169 -- 3.019649  (5)
		117: 7.208786 -- 0.866752  (110)
		118: 7.334330 -- 2.264250  (15)
		119: 6.316892 -- 1.422192  (22)
		120: 7.779847 -- 1.323524  (41)
		121: 8.510600 -- 1.803075  (27)
		122: 8.181498 -- 2.255214  (15)
	--> std: 1.725679 -- 0.587089

	5: 6.929336 (157)
		123: 7.203831 -- 3.603129  (5)
		124: 5.385010 -- 2.059791  (8)
		125: 4.799666 -- 2.157709  (6)
		126: 6.161561 -- 1.523064  (21)
		127: 7.270220 -- 1.120937  (117)
	--> std: 1.586482 -- 0.843269

	Avg: 6.667373 -- 0.765806 
	Std: 2.769887 -- 2.653519 

Subclusters Formed: 128

Selecting Representatives:
Representatives:
[9, 3, 0, 1, 0, 0, 0, 14, 2, 12, 1, 2, 2, 2, 7, 1, 4, 4, 6, 11, 0, 7, 8, 0, 0, 0, 16, 2, 5, 101, 0, 10, 0, 20, 0, 7, 1, 0, 2, 2, 0, 0, 0, 0, 4, 7, 11, 0, 1, 2, 8, 6, 34, 0, 12, 2, 0, 14, 10, 2, 0, 0, 0, 3, 4, 2, 10, 6, 12, 3, 6, 1, 2, 0, 0, 0, 6, 0, 6, 0, 0, 0, 8, 1, 0, 0, 3, 0, 2, 0, 5, 0, 1, 18, 0, 2, 3, 1, 5, 5, 1, 3, 0, 25, 218, 2, 4, 19, 0, 0, 5, 0, 0, 6, 0, 3, 0, 7, 3, 3, 6, 3, 0, 0, 1, 0, 11, 32]


Generating Feedback:
Farthest First
Computing pairwise distances between representatives.
Beginning Queries
Feedback: 1
	{'Type1': [119]}
Number of Queries: 1

Merging Subclusters:
Generating Data

Train All

Merged Feedback:
	[[119]] 1

Connot Link Subcluster Constraints:
	[set([])] 1

Test All

Creating files for: _deploy
Creating Pairs
Train_data: (22, 403)
Data: (4974, 403)
sims: (240,)
Creating files for: _train
Network:
layer {
  name: "data"
  type: "HDF5Data"
  top: "data"
  top: "data_p"
  top: "sims"
  hdf5_data_param {
    source: "_train.txt"
    batch_size: 10
    shuffle: true
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "data"
  top: "ip1"
  param {
    name: "ip1_w"
    lr_mult: 1
  }
  param {
    name: "ip1_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "gaussian"
      std: 0.1
    }
  }
}
layer {
  name: "s1"
  type: "Sigmoid"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "feat"
  type: "InnerProduct"
  bottom: "ip1"
  top: "feat"
  param {
    name: "feat_w"
    lr_mult: 1
  }
  param {
    name: "feat_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 100
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "ip1_p"
  type: "InnerProduct"
  bottom: "data_p"
  top: "ip1_p"
  param {
    name: "ip1_w"
    lr_mult: 1
  }
  param {
    name: "ip1_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "gaussian"
      std: 0.1
    }
  }
}
layer {
  name: "s1_p"
  type: "Sigmoid"
  bottom: "ip1_p"
  top: "ip1_p"
}
layer {
  name: "feat_p"
  type: "InnerProduct"
  bottom: "ip1_p"
  top: "feat_p"
  param {
    name: "feat_w"
    lr_mult: 1
  }
  param {
    name: "feat_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 100
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "ContrastiveLoss"
  bottom: "feat"
  bottom: "feat_p"
  bottom: "sims"
  top: "loss"
  contrastive_loss_param {
    margin: 1
  }
}


Training siamese network
data (4974, 403)
targets: (4974,)
K: 1
Reclustering

~! 1 !~
Accuracy: 2263 of 4974: 45.497 %
H: 0.000000 C: 1.000000 V: 0.000000 JCC: 0.364207



~! 2 !~
Accuracy: 3564 of 4974: 71.653 %
H: 0.232457 C: 0.395855 V: 0.292910 JCC: 0.457375



~! 3 !~
Accuracy: 3861 of 4974: 77.624 %
H: 0.336131 C: 0.573314 V: 0.423794 JCC: 0.559822



~! 4 !~
Accuracy: 3776 of 4974: 75.915 %
H: 0.308134 C: 0.526597 V: 0.388778 JCC: 0.524168



~! 5 !~
Accuracy: 4022 of 4974: 80.860 %
H: 0.445679 C: 0.477943 V: 0.461247 JCC: 0.521543



~! 6 !~
Accuracy: 3941 of 4974: 79.232 %
H: 0.449353 C: 0.478455 V: 0.463448 JCC: 0.503708



~! 7 !~
Accuracy: 3983 of 4974: 80.076 %
H: 0.408205 C: 0.688653 V: 0.512576 JCC: 0.612525



~! 8 !~
Accuracy: 4000 of 4974: 80.418 %
H: 0.470187 C: 0.500380 V: 0.484814 JCC: 0.522822



~! 9 !~
Accuracy: 4021 of 4974: 80.840 %
H: 0.467121 C: 0.496175 V: 0.481210 JCC: 0.516349



~! 10 !~
Accuracy: 4028 of 4974: 80.981 %
H: 0.494421 C: 0.527990 V: 0.510654 JCC: 0.550509



~! 11 !~
Accuracy: 4001 of 4974: 80.438 %
H: 0.473640 C: 0.505850 V: 0.489216 JCC: 0.539211



~! 12 !~
Accuracy: 3975 of 4974: 79.916 %
H: 0.483406 C: 0.517583 V: 0.499911 JCC: 0.536797



~! 13 !~
Accuracy: 4061 of 4974: 81.645 %
H: 0.494631 C: 0.526608 V: 0.510119 JCC: 0.534476



~! 14 !~
Accuracy: 4035 of 4974: 81.122 %
H: 0.502492 C: 0.543571 V: 0.522225 JCC: 0.573463



~! 15 !~
Accuracy: 3890 of 4974: 78.207 %
H: 0.491177 C: 0.432028 V: 0.459708 JCC: 0.472954



~! 16 !~
Accuracy: 4032 of 4974: 81.062 %
H: 0.506204 C: 0.540689 V: 0.522878 JCC: 0.555020



~! 17 !~
Accuracy: 4296 of 4974: 86.369 %
H: 0.579162 C: 0.533848 V: 0.555582 JCC: 0.575715



~! 18 !~
Accuracy: 4073 of 4974: 81.886 %
H: 0.537596 C: 0.468807 V: 0.500851 JCC: 0.516519



~! 19 !~
Accuracy: 4056 of 4974: 81.544 %
H: 0.535011 C: 0.470657 V: 0.500775 JCC: 0.515354



~! 20 !~
Accuracy: 4034 of 4974: 81.102 %
H: 0.525914 C: 0.577533 V: 0.550516 JCC: 0.607997



~! 21 !~
Accuracy: 4056 of 4974: 81.544 %
H: 0.501624 C: 0.545149 V: 0.522482 JCC: 0.563924



~! 22 !~
Accuracy: 4058 of 4974: 81.584 %
H: 0.515852 C: 0.580169 V: 0.546123 JCC: 0.599636



~! 23 !~
Accuracy: 4234 of 4974: 85.123 %
H: 0.600073 C: 0.475794 V: 0.530755 JCC: 0.527636



~! 24 !~
Accuracy: 4041 of 4974: 81.242 %
H: 0.559187 C: 0.518164 V: 0.537895 JCC: 0.586223



~! 25 !~
Accuracy: 4102 of 4974: 82.469 %
H: 0.573574 C: 0.516174 V: 0.543362 JCC: 0.569718



~! 26 !~
Accuracy: 4029 of 4974: 81.001 %
H: 0.568702 C: 0.530182 V: 0.548767 JCC: 0.581963



~! 27 !~
Accuracy: 4068 of 4974: 81.785 %
H: 0.510134 C: 0.571061 V: 0.538881 JCC: 0.597556



~! 28 !~
Accuracy: 3994 of 4974: 80.298 %
H: 0.569654 C: 0.517636 V: 0.542401 JCC: 0.557110



~! 29 !~
Accuracy: 4060 of 4974: 81.624 %
H: 0.595035 C: 0.558360 V: 0.576114 JCC: 0.625013



~! 30 !~
Accuracy: 4045 of 4974: 81.323 %
H: 0.518054 C: 0.461841 V: 0.488335 JCC: 0.507122



~! 31 !~
Accuracy: 4035 of 4974: 81.122 %
H: 0.589215 C: 0.543621 V: 0.565500 JCC: 0.600451



~! 32 !~
Accuracy: 4170 of 4974: 83.836 %
H: 0.549115 C: 0.491786 V: 0.518872 JCC: 0.535306



~! 33 !~
Accuracy: 4185 of 4974: 84.138 %
H: 0.616941 C: 0.599183 V: 0.607933 JCC: 0.673738



~! 34 !~
Accuracy: 4040 of 4974: 81.222 %
H: 0.578914 C: 0.543182 V: 0.560479 JCC: 0.601347



~! 35 !~
Accuracy: 4200 of 4974: 84.439 %
H: 0.603425 C: 0.575727 V: 0.589251 JCC: 0.651599



~! 36 !~
Accuracy: 4045 of 4974: 81.323 %
H: 0.592656 C: 0.549567 V: 0.570299 JCC: 0.599586



~! 37 !~
Accuracy: 4138 of 4974: 83.193 %
H: 0.621777 C: 0.591956 V: 0.606500 JCC: 0.659006



~! 38 !~
Accuracy: 4213 of 4974: 84.700 %
H: 0.556495 C: 0.503061 V: 0.528431 JCC: 0.545770



~! 39 !~
Accuracy: 4215 of 4974: 84.741 %
H: 0.607009 C: 0.580024 V: 0.593210 JCC: 0.654751



~! 40 !~
Accuracy: 4264 of 4974: 85.726 %
H: 0.565223 C: 0.511831 V: 0.537204 JCC: 0.553680



~! 41 !~
Accuracy: 4205 of 4974: 84.540 %
H: 0.612439 C: 0.595973 V: 0.604094 JCC: 0.669923



~! 42 !~
Accuracy: 4234 of 4974: 85.123 %
H: 0.636123 C: 0.617982 V: 0.626922 JCC: 0.686009



~! 43 !~
Accuracy: 4188 of 4974: 84.198 %
H: 0.575504 C: 0.535214 V: 0.554628 JCC: 0.610353



~! 44 !~
Accuracy: 4173 of 4974: 83.896 %
H: 0.542263 C: 0.488278 V: 0.513856 JCC: 0.535617



~! 45 !~
Accuracy: 4131 of 4974: 83.052 %
H: 0.600349 C: 0.575637 V: 0.587733 JCC: 0.647378



~! 46 !~
Accuracy: 4158 of 4974: 83.595 %
H: 0.603324 C: 0.586007 V: 0.594540 JCC: 0.658766



~! 47 !~
Accuracy: 4174 of 4974: 83.916 %
H: 0.609128 C: 0.586581 V: 0.597642 JCC: 0.660918



~! 48 !~
Accuracy: 4189 of 4974: 84.218 %
H: 0.619098 C: 0.597599 V: 0.608159 JCC: 0.669584



~! 49 !~
Accuracy: 4083 of 4974: 82.087 %
H: 0.592946 C: 0.564162 V: 0.578196 JCC: 0.629146



~! 50 !~
Accuracy: 4223 of 4974: 84.901 %
H: 0.589451 C: 0.579333 V: 0.584348 JCC: 0.657370



~! 51 !~
Accuracy: 4212 of 4974: 84.680 %
H: 0.617056 C: 0.609419 V: 0.613214 JCC: 0.680485



~! 52 !~
Accuracy: 4162 of 4974: 83.675 %
H: 0.571535 C: 0.549164 V: 0.560126 JCC: 0.621459



~! 53 !~
Accuracy: 4193 of 4974: 84.298 %
H: 0.615125 C: 0.606367 V: 0.610715 JCC: 0.675664



~! 54 !~
Accuracy: 4221 of 4974: 84.861 %
H: 0.618752 C: 0.605786 V: 0.612200 JCC: 0.680214



~! 55 !~
Accuracy: 4205 of 4974: 84.540 %
H: 0.611901 C: 0.601142 V: 0.606474 JCC: 0.674621



~! 56 !~
Accuracy: 4220 of 4974: 84.841 %
H: 0.625743 C: 0.571124 V: 0.597187 JCC: 0.621763



~! 57 !~
Accuracy: 4090 of 4974: 82.228 %
H: 0.575289 C: 0.506424 V: 0.538664 JCC: 0.537420



~! 58 !~
Accuracy: 4242 of 4974: 85.283 %
H: 0.633397 C: 0.497547 V: 0.557313 JCC: 0.550995



~! 59 !~
Accuracy: 4321 of 4974: 86.872 %
H: 0.649584 C: 0.609934 V: 0.629135 JCC: 0.664194



~! 60 !~
Accuracy: 4090 of 4974: 82.228 %
H: 0.559447 C: 0.495965 V: 0.525797 JCC: 0.530705



~! 61 !~
Accuracy: 4088 of 4974: 82.187 %
H: 0.566979 C: 0.505650 V: 0.534561 JCC: 0.546848



~! 62 !~
Accuracy: 4236 of 4974: 85.163 %
H: 0.618651 C: 0.614972 V: 0.616806 JCC: 0.686778



~! 63 !~
Accuracy: 4258 of 4974: 85.605 %
H: 0.614716 C: 0.615833 V: 0.615274 JCC: 0.687315



~! 64 !~
Accuracy: 4263 of 4974: 85.706 %
H: 0.624525 C: 0.630152 V: 0.627326 JCC: 0.701546



~! 65 !~
Accuracy: 4314 of 4974: 86.731 %
H: 0.635412 C: 0.518889 V: 0.571269 JCC: 0.570296



~! 66 !~
Accuracy: 4199 of 4974: 84.419 %
H: 0.601509 C: 0.496415 V: 0.543932 JCC: 0.556949



~! 67 !~
Accuracy: 4315 of 4974: 86.751 %
H: 0.647169 C: 0.521268 V: 0.577435 JCC: 0.566847



~! 68 !~
Accuracy: 4313 of 4974: 86.711 %
H: 0.635620 C: 0.516899 V: 0.570145 JCC: 0.564640



~! 69 !~
Accuracy: 4275 of 4974: 85.947 %
H: 0.638652 C: 0.510849 V: 0.567646 JCC: 0.571476



~! 70 !~
Accuracy: 4243 of 4974: 85.304 %
H: 0.635319 C: 0.512466 V: 0.567317 JCC: 0.567144



~! 71 !~
Accuracy: 4364 of 4974: 87.736 %
H: 0.667894 C: 0.535105 V: 0.594171 JCC: 0.581947



~! 72 !~
Accuracy: 4355 of 4974: 87.555 %
H: 0.662183 C: 0.528641 V: 0.587924 JCC: 0.578967



~! 73 !~
Accuracy: 4339 of 4974: 87.234 %
H: 0.657082 C: 0.523530 V: 0.582752 JCC: 0.574967



~! 74 !~
Accuracy: 4276 of 4974: 85.967 %
H: 0.631611 C: 0.498910 V: 0.557472 JCC: 0.550665



~! 75 !~
Accuracy: 4339 of 4974: 87.234 %
H: 0.651191 C: 0.528952 V: 0.583741 JCC: 0.595366



~! 76 !~
Accuracy: 4326 of 4974: 86.972 %
H: 0.666958 C: 0.531861 V: 0.591797 JCC: 0.579985



~! 77 !~
Accuracy: 4363 of 4974: 87.716 %
H: 0.669565 C: 0.533086 V: 0.593581 JCC: 0.579616



~! 78 !~
Accuracy: 4301 of 4974: 86.470 %
H: 0.651708 C: 0.519798 V: 0.578327 JCC: 0.570361



~! 79 !~
Accuracy: 4398 of 4974: 88.420 %
H: 0.684568 C: 0.554054 V: 0.612435 JCC: 0.605055



~! 80 !~
Accuracy: 4356 of 4974: 87.575 %
H: 0.667680 C: 0.538826 V: 0.596373 JCC: 0.593584



~! 81 !~
Accuracy: 4363 of 4974: 87.716 %
H: 0.666116 C: 0.537021 V: 0.594642 JCC: 0.590871



~! 82 !~
Accuracy: 4375 of 4974: 87.957 %
H: 0.680438 C: 0.548368 V: 0.607305 JCC: 0.594181



~! 83 !~
Accuracy: 4332 of 4974: 87.093 %
H: 0.656511 C: 0.529321 V: 0.586095 JCC: 0.585364



~! 84 !~
Accuracy: 4334 of 4974: 87.133 %
H: 0.657339 C: 0.538063 V: 0.591750 JCC: 0.604647



~! 85 !~
Accuracy: 4342 of 4974: 87.294 %
H: 0.665455 C: 0.534910 V: 0.593084 JCC: 0.591974



~! 86 !~
Accuracy: 4367 of 4974: 87.797 %
H: 0.672664 C: 0.549125 V: 0.604649 JCC: 0.617943



~! 87 !~
Accuracy: 4366 of 4974: 87.776 %
H: 0.678288 C: 0.549803 V: 0.607324 JCC: 0.606696



~! 88 !~
Accuracy: 4400 of 4974: 88.460 %
H: 0.680156 C: 0.560317 V: 0.614448 JCC: 0.630651



~! 89 !~
Accuracy: 4358 of 4974: 87.616 %
H: 0.663763 C: 0.529458 V: 0.589052 JCC: 0.572956



~! 90 !~
Accuracy: 4370 of 4974: 87.857 %
H: 0.678247 C: 0.552608 V: 0.609015 JCC: 0.618480



~! 91 !~
Accuracy: 4395 of 4974: 88.359 %
H: 0.681520 C: 0.557449 V: 0.613272 JCC: 0.620504



~! 92 !~
Accuracy: 4380 of 4974: 88.058 %
H: 0.681445 C: 0.554492 V: 0.611448 JCC: 0.618725



~! 93 !~
Accuracy: 4340 of 4974: 87.254 %
H: 0.664564 C: 0.536793 V: 0.593884 JCC: 0.592527



~! 94 !~
Accuracy: 4401 of 4974: 88.480 %
H: 0.682683 C: 0.568151 V: 0.620174 JCC: 0.640690



~! 95 !~
Accuracy: 4358 of 4974: 87.616 %
H: 0.663678 C: 0.542744 V: 0.597150 JCC: 0.613323



~! 96 !~
Accuracy: 4405 of 4974: 88.561 %
H: 0.689099 C: 0.571679 V: 0.624921 JCC: 0.640503



~! 97 !~
Accuracy: 4373 of 4974: 87.917 %
H: 0.667140 C: 0.534956 V: 0.593781 JCC: 0.584855



~! 98 !~
Accuracy: 4392 of 4974: 88.299 %
H: 0.681547 C: 0.550768 V: 0.609218 JCC: 0.604129



~! 99 !~
Accuracy: 4424 of 4974: 88.943 %
H: 0.695921 C: 0.561272 V: 0.621385 JCC: 0.611329



~! 100 !~
Accuracy: 4383 of 4974: 88.118 %
H: 0.679487 C: 0.556368 V: 0.611795 JCC: 0.618647



~! 101 !~
Accuracy: 4408 of 4974: 88.621 %
H: 0.689229 C: 0.570449 V: 0.624239 JCC: 0.638466



~! 102 !~
Accuracy: 4429 of 4974: 89.043 %
H: 0.700449 C: 0.567230 V: 0.626840 JCC: 0.616678



~! 103 !~
Accuracy: 4406 of 4974: 88.581 %
H: 0.689958 C: 0.557760 V: 0.616856 JCC: 0.605086



~! 104 !~
Accuracy: 4425 of 4974: 88.963 %
H: 0.691650 C: 0.565198 V: 0.622063 JCC: 0.622393



~! 105 !~
Accuracy: 4424 of 4974: 88.943 %
H: 0.698528 C: 0.583216 V: 0.635685 JCC: 0.651685



~! 106 !~
Accuracy: 4405 of 4974: 88.561 %
H: 0.688577 C: 0.560002 V: 0.617669 JCC: 0.613075



~! 107 !~
Accuracy: 4385 of 4974: 88.158 %
H: 0.670764 C: 0.550879 V: 0.604939 JCC: 0.620263



~! 108 !~
Accuracy: 4411 of 4974: 88.681 %
H: 0.683127 C: 0.574078 V: 0.623873 JCC: 0.648527



~! 109 !~
Accuracy: 4428 of 4974: 89.023 %
H: 0.692995 C: 0.565530 V: 0.622807 JCC: 0.622794



~! 110 !~
Accuracy: 4424 of 4974: 88.943 %
H: 0.693325 C: 0.576692 V: 0.629653 JCC: 0.646849



~! 111 !~
Accuracy: 4423 of 4974: 88.922 %
H: 0.691153 C: 0.569492 V: 0.624452 JCC: 0.635527



~! 112 !~
Accuracy: 4394 of 4974: 88.339 %
H: 0.680281 C: 0.564327 V: 0.616903 JCC: 0.637634



~! 113 !~
Accuracy: 4420 of 4974: 88.862 %
H: 0.687853 C: 0.560302 V: 0.617560 JCC: 0.614329



~! 114 !~
Accuracy: 4429 of 4974: 89.043 %
H: 0.697790 C: 0.567497 V: 0.625935 JCC: 0.619784



~! 115 !~
Accuracy: 4411 of 4974: 88.681 %
H: 0.686579 C: 0.563783 V: 0.619151 JCC: 0.625707



~! 116 !~
Accuracy: 4417 of 4974: 88.802 %
H: 0.689577 C: 0.569868 V: 0.624033 JCC: 0.633186



~! 117 !~
Accuracy: 4437 of 4974: 89.204 %
H: 0.696374 C: 0.575771 V: 0.630356 JCC: 0.639304



~! 118 !~
Accuracy: 4440 of 4974: 89.264 %
H: 0.694444 C: 0.563266 V: 0.622014 JCC: 0.615589



~! 119 !~
Accuracy: 4443 of 4974: 89.324 %
H: 0.699253 C: 0.573643 V: 0.630251 JCC: 0.634421



~! 120 !~
Accuracy: 4408 of 4974: 88.621 %
H: 0.685082 C: 0.567214 V: 0.620601 JCC: 0.635555



~! 121 !~
Accuracy: 4444 of 4974: 89.345 %
H: 0.699118 C: 0.561959 V: 0.623080 JCC: 0.607275



~! 122 !~
Accuracy: 4435 of 4974: 89.164 %
H: 0.695476 C: 0.578866 V: 0.631836 JCC: 0.640305



~! 123 !~
Accuracy: 4427 of 4974: 89.003 %
H: 0.694976 C: 0.570579 V: 0.626664 JCC: 0.631933



~! 124 !~
Accuracy: 4428 of 4974: 89.023 %
H: 0.696846 C: 0.571809 V: 0.628165 JCC: 0.632886



~! 125 !~
Accuracy: 4434 of 4974: 89.144 %
H: 0.697194 C: 0.577641 V: 0.631812 JCC: 0.641885



~! 126 !~
Accuracy: 4457 of 4974: 89.606 %
H: 0.705191 C: 0.576210 V: 0.634209 JCC: 0.630503



~! 127 !~
Accuracy: 4438 of 4974: 89.224 %
H: 0.703703 C: 0.584324 V: 0.638482 JCC: 0.652428


INCREMENT: (127)
SubClusters: 128
Accuracy: 4320 of 4974: 86.852 %
H: 0.687864 C: 0.182566 V: 0.288548 JCC: 0.039232


Final
Accuracy: 4438 of 4974: 89.224 %
H: 0.703703 C: 0.584324 V: 0.638482 JCC: 0.652428

Rows are labels, Columns are Clusters

        0     1    2    3    4
  0     0    29    0   35   27
  1     0    94  358   64   62
  2  1864     0    0   11    2
  3     0     2    0  130   33
  4     6  1575   41  229  412



Total Time: 1 h 19 m 5.515400 s




Solver:
net: "_TRAIN_NET.prototxt"
base_lr: 0.1
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.1
max_iter: 10000
display: 1000
weight_decay: 0.000000
solver_mode: GPU
